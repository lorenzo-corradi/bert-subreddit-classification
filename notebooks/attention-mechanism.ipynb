{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder representations of three different words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_1 = np.array([1, 0, 0, 2, 0])\n",
    "word_2 = np.array([0, 1, 0, 0, 1])\n",
    "word_3 = np.array([1, 1, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the weight matrices <br> These are the matrices to train <br> They are trained in order to build an inquiry system: what is 'key' for the word 'query'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "W_Q = np.random.randint(low=2, size=(5, 3))\n",
    "W_K = np.random.randint(low=2, size=(5, 3))\n",
    "W_V = np.random.randint(low=2, size=(5, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the queries, keys and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = word_1 @ W_Q\n",
    "key_1 = word_1 @ W_K\n",
    "value_1 = word_1 @ W_V\n",
    " \n",
    "query_2 = word_2 @ W_Q\n",
    "key_2 = word_2 @ W_K\n",
    "value_2 = word_2 @ W_V\n",
    " \n",
    "query_3 = word_3 @ W_Q\n",
    "key_3 = word_3 @ W_K\n",
    "value_3 = word_3 @ W_V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dot product is a similarity score between queries and keys <br> In reality we use a fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_1 = np.array([np.dot(query_1, key_1), np.dot(query_1, key_2), np.dot(query_1, key_3)])\n",
    "scores_2 = np.array([np.dot(query_2, key_1), np.dot(query_2, key_2), np.dot(query_2, key_3)])\n",
    "scores_3 = np.array([np.dot(query_3, key_1), np.dot(query_3, key_2), np.dot(query_3, key_3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the weights by a softmax operation (can be thought as a probability vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1361258 0.4319371 0.4319371]\n",
      "[0.60215404 0.05980637 0.33803959]\n",
      "[0.60215404 0.05980637 0.33803959]\n"
     ]
    }
   ],
   "source": [
    "weights_1 = softmax(scores_1 / key_1.shape[0] ** 0.5)\n",
    "weights_2 = softmax(scores_2 / key_2.shape[0] ** 0.5)\n",
    "weights_3 = softmax(scores_3 / key_3.shape[0] ** 0.5)\n",
    "\n",
    "print(weights_1)\n",
    "print(weights_2)\n",
    "print(weights_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the attention by a weighted sum of the value vectors <br> Can be thought of doing 'proportional retrieval' according to the probability vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_1 = (weights_1[0] * value_1) + (weights_1[1] * value_2) + (weights_1[2] * value_3)\n",
    "attention_2 = (weights_2[0] * value_1) + (weights_2[1] * value_2) + (weights_2[2] * value_3)\n",
    "attention_3 = (weights_3[0] * value_1) + (weights_3[1] * value_2) + (weights_3[2] * value_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.7041887 0.2722516 0.2722516]\n",
      "[2.54234767 1.20430808 1.20430808]\n",
      "[2.54234767 1.20430808 1.20430808]\n"
     ]
    }
   ],
   "source": [
    "print(attention_1)\n",
    "print(attention_2)\n",
    "print(attention_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All these operations can be summarized into this formula: <br> $\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{n}})\\mathbf{V}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer uses word vectors as a set of key-value pairs <br> the query is obtained by compressing the output at time t-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moreover it applies the attention operation we saw multiple times in parallel in this way: <br> $\\begin{aligned} \\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) &= [\\text{head}_1; \\dots; \\text{head}_h]\\mathbf{W}^O \\\\ \\text{where head}_i &= \\text{Attention}(\\mathbf{Q}\\mathbf{W}^Q_i, \\mathbf{K}\\mathbf{W}^K_i, \\mathbf{V}\\mathbf{W}^V_i) \\end{aligned} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"..\\\\images\\\\transformer_full.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I focused particularly on the MultiHead Attention aspect since it's the core of the Transformer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('machine_learning_lab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f876acc5802b5e3b88cbc4e26e8dff4ccb386f485843c0853522128b4d62494c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
