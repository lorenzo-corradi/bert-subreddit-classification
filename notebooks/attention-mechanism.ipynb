{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"..\\\\images\\\\transformer_full.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder representations of three different words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_1 = np.array([1, 0, 0, 1, 0, 1, 0, 0])\n",
    "word_2 = np.array([0, 1, 0, 0, 1, 1, 0, 1])\n",
    "word_3 = np.array([1, 1, 0, 0, 1, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the weight matrices <br> These are the matrices to train <br> They are trained in order to build an inquiry system: what is 'key' for the word 'query'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "W_Q = np.random.randint(low=3, size=(8, 3))\n",
    "W_K = np.random.randint(low=3, size=(8, 3))\n",
    "W_V = np.random.randint(low=3, size=(8, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the queries, keys and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = word_1 @ W_Q\n",
    "key_1 = word_1 @ W_K\n",
    "value_1 = word_1 @ W_V\n",
    " \n",
    "query_2 = word_2 @ W_Q\n",
    "key_2 = word_2 @ W_K\n",
    "value_2 = word_2 @ W_V\n",
    " \n",
    "query_3 = word_3 @ W_Q\n",
    "key_3 = word_3 @ W_K\n",
    "value_3 = word_3 @ W_V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dot product is a similarity score between queries and keys <br> In reality we use a fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_1 = np.array([np.dot(query_1, key_1), np.dot(query_1, key_2), np.dot(query_1, key_3)])\n",
    "scores_2 = np.array([np.dot(query_2, key_1), np.dot(query_2, key_2), np.dot(query_2, key_3)])\n",
    "scores_3 = np.array([np.dot(query_3, key_1), np.dot(query_3, key_2), np.dot(query_3, key_3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the weights by a softmax operation (can be thought as a probability vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49754595 0.00490809 0.49754595]\n",
      "[4.9975518e-01 4.8963998e-04 4.9975518e-01]\n",
      "[9.03393582e-02 8.85108617e-05 9.09572131e-01]\n"
     ]
    }
   ],
   "source": [
    "weights_1 = softmax(scores_1 / key_1.shape[0] ** 0.5)\n",
    "weights_2 = softmax(scores_2 / key_2.shape[0] ** 0.5)\n",
    "weights_3 = softmax(scores_3 / key_3.shape[0] ** 0.5)\n",
    "\n",
    "print(weights_1)\n",
    "print(weights_2)\n",
    "print(weights_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the attention by a weighted sum of the value vectors <br> Can be thought of doing 'proportional retrieval' according to the probability vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_1 = (weights_1[0] * value_1) + (weights_1[1] * value_2) + (weights_1[2] * value_3)\n",
    "attention_2 = (weights_2[0] * value_1) + (weights_2[1] * value_2) + (weights_2[2] * value_3)\n",
    "attention_3 = (weights_3[0] * value_1) + (weights_3[1] * value_2) + (weights_3[2] * value_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.49754595 0.50245405 6.50245405]\n",
      "[6.49975518 0.50024482 6.50024482]\n",
      "[6.90957213 0.90966064 7.72889341]\n"
     ]
    }
   ],
   "source": [
    "print(attention_1)\n",
    "print(attention_2)\n",
    "print(attention_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To summarize:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input: $\\mathbf{X}\\in\\mathbf{R}^{batch \\times dim}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainable weight matrices: $\\mathbf{W}_i^{Q},\\mathbf{W}_i^{K},\\mathbf{W}_i^{V}\\in\\mathbf{R}^{d_{model}\\times d_k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We create three different representations: $\\mathbf{Q} = \\mathbf{X}\\mathbf{W}_Q, \\mathbf{K} = \\mathbf{X}\\mathbf{W}_K, \\mathbf{V} = \\mathbf{X}\\mathbf{W}_V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All these operations can be summarized into this formula: <br> $\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{n}})\\mathbf{V}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The attention operation can be applied multiple times in parallel in this way: <br> $\\begin{aligned} \\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) &= [\\text{head}_1; \\dots; \\text{head}_h]\\mathbf{W}^O \\\\ \\text{where head}_i &= \\text{Attention}(\\mathbf{Q}\\mathbf{W}^Q_i, \\mathbf{K}\\mathbf{W}^K_i, \\mathbf{V}\\mathbf{W}^V_i) \\end{aligned} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"..\\\\images\\\\multi-head-attention-peltarion.png\" width=\"500\"/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('machine_learning_lab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f876acc5802b5e3b88cbc4e26e8dff4ccb386f485843c0853522128b4d62494c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
